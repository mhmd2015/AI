{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+F+B0ixVIFYmgHxRtpjxJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhmd2015/AI/blob/main/UIC421Week1NPL1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.1: Load the dataset in a pandas DataFrame; use only **foxnewshealth.txt** and **cnnhealth.txt** for next tasks. Use the tweets column only, for further tasks."
      ],
      "metadata": {
        "id": "JbXZh3WBpb5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def loadDS(file_path):\n",
        "  try:\n",
        "    df = pd.read_csv(file_path, sep='|', header=None, names=['tweet_id', 'date', 'tweet'], encoding='latin1',error_bad_lines=False, warn_bad_lines=True)\n",
        "    print('latin1')\n",
        "  except UnicodeDecodeError:\n",
        "    df = pd.read_csv(file_path, sep='|', header=None, names=['tweet_id', 'date', 'tweet'], encoding='ISO-8859-1',error_bad_lines=False, warn_bad_lines=True)\n",
        "    print('ISO-8859-1')\n",
        "  except UnicodeDecodeError:\n",
        "    df = pd.read_csv(file_path, sep='|', header=None, names=['tweet_id', 'date', 'tweet'], encoding='cp1252',error_bad_lines=False, warn_bad_lines=True)\n",
        "    print('cp1252')\n",
        "  except UnicodeDecodeError:\n",
        "    print(file_path+' has error')\n",
        "\n",
        "  print(len(df))\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "idzx8Mm-n3bZ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = loadDS('foxnewshealth.txt')\n",
        "#print(df1.head())\n",
        "#print(df1['tweet'])\n",
        "\n",
        "df2 = loadDS('cnnhealth.txt')\n",
        "#print(df2.head())\n",
        "#print(df2['tweet'])\n",
        "\n",
        "df = pd.concat([df1, df2], ignore_index=True)\n",
        "#print(len(df))\n",
        "#print(df['tweet'])"
      ],
      "metadata": {
        "id": "VOWSJIJhok6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.2 : Using regular expressions, extract hashtags in the tweets and store them. Keep a counter for each hashtag."
      ],
      "metadata": {
        "id": "JVZ8YCLxtcYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Assuming df is your DataFrame and it has the 'text' column with tweets\n",
        "\n",
        "# Extract and count hashtags\n",
        "hashtag_counts = {}\n",
        "for text in df['tweet']:\n",
        "    found_hashtags = re.findall(r\"#(\\w+)\", text)\n",
        "    for hashtag in found_hashtags:\n",
        "        if hashtag in hashtag_counts:\n",
        "            hashtag_counts[hashtag] += 1\n",
        "        else:\n",
        "            hashtag_counts[hashtag] = 1\n",
        "\n",
        "# Display the counts\n",
        "for hashtag, count in hashtag_counts.items():\n",
        "    print(f\"#{hashtag}: {count}\")\n"
      ],
      "metadata": {
        "id": "Ztu0P9Djuw_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.3:  Then sentence tokenize each tweet and then word tokenize each sentence. Keep a counter of each word and sort them in descending order. Notice that there are a lot of special characters and repeated words with different cases, like ‘New’ vs ‘new’.\n"
      ],
      "metadata": {
        "id": "aUQcbCAr0Ll2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tokenize_and_count_words_re(texts):\n",
        "    word_counts = {}\n",
        "\n",
        "    # Regular expression patterns\n",
        "    sentence_pattern = r'[.!?]+'\n",
        "    word_pattern = r'\\b\\w+\\b'\n",
        "\n",
        "    for text in texts:\n",
        "        # Sentence tokenization\n",
        "        sentences = re.split(sentence_pattern, text)\n",
        "        for sentence in sentences:\n",
        "            # Word tokenization\n",
        "            words = re.findall(word_pattern, sentence)\n",
        "            for word in words:\n",
        "                cleaned_word = word.lower()  # Lowercase the word for uniformity\n",
        "                if cleaned_word in word_counts:\n",
        "                    word_counts[cleaned_word] += 1\n",
        "                else:\n",
        "                    word_counts[cleaned_word] = 1\n",
        "\n",
        "    # Sort the word counts dictionary in descending order of counts\n",
        "    sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return sorted_word_counts\n"
      ],
      "metadata": {
        "id": "PwN3QwASKqD9"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_word_counts = tokenize_and_count_words_re(df['tweet'])\n",
        "\n",
        "#for word, count in sorted_word_counts:\n",
        "#    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "Ct5asUfg0g1B"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('words_re.txt', 'w') as file:\n",
        "    for word, count in sorted_word_counts:\n",
        "        file.write(f\"{word}: {count}\\n\")"
      ],
      "metadata": {
        "id": "q35wwsFE1a1L"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.4 Clean the tweets by removing the special characters and unwanted strings like numbers, HTTP links, etc. Use regular expressions to do so. Normalize the case of all the characters in all the tweets."
      ],
      "metadata": {
        "id": "M0W-eSpe9zYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_tweets(texts):\n",
        "    cleaned_texts = []\n",
        "\n",
        "    for text in texts:\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        # Remove special characters and numbers\n",
        "        text = re.sub(r'\\W+|\\d+', ' ', text)\n",
        "\n",
        "        # Normalize case\n",
        "        text = text.lower()\n",
        "\n",
        "        cleaned_texts.append(text)\n",
        "\n",
        "    return cleaned_texts\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'df' is your DataFrame with a 'text' column containing tweets\n",
        "# cleaned_tweets = clean_tweets(df['text'])\n",
        "\n",
        "# Now 'cleaned_tweets' contains the cleaned and normalized tweets\n"
      ],
      "metadata": {
        "id": "ChUzKY7G90Aw"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_tweets = clean_tweets(df['tweet'])\n",
        "print(cleaned_tweets)\n",
        "\n",
        "with open('clean_tweets.txt', 'w') as file:\n",
        "    for text in cleaned_tweets:\n",
        "        file.write(f\"{text}\\n\")"
      ],
      "metadata": {
        "id": "U0kwBfr5974t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.5 Remove Stopwords from all the text examples. You may use any library like SpaCy or NLTK. Repeat task 3 and notice the difference."
      ],
      "metadata": {
        "id": "oFjCZdp3o8pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VFnH5K6go-lp",
        "outputId": "0309ba7d-3023-4498-ddfa-722c584ea862"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    cleaned_texts = []\n",
        "    for text in texts:\n",
        "        tokens = word_tokenize(text)\n",
        "        filtered_text = ' '.join([word for word in tokens if word not in stop_words])\n",
        "        cleaned_texts.append(filtered_text)\n",
        "\n",
        "    return cleaned_texts\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'cleaned_tweets' contains the cleaned and normalized tweets\n",
        "tweets_without_stopwords = remove_stopwords(cleaned_tweets)\n",
        "\n",
        "with open('tweets_without_stopwords.txt', 'w') as file:\n",
        "    for text in cleaned_tweets:\n",
        "        file.write(f\"{text}\\n\")\n"
      ],
      "metadata": {
        "id": "urU0tUorpAnx"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.6 Lemmatize the words in step 5 to their base form using either NLTK or SpaCy. Use stemming to the same words in 5 and observe the difference."
      ],
      "metadata": {
        "id": "ZcVCwTebJRRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install nltk\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "z9tXAbZFJTY3",
        "outputId": "1befbacb-d5f4-47f6-dfc9-2b00837c7d05"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def lemmatize_tweets(tweets):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tweets = []\n",
        "    for tweet in tweets:\n",
        "        words = word_tokenize(tweet)\n",
        "        lemmatized_tweet = ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
        "        lemmatized_tweets.append(lemmatized_tweet)\n",
        "    return lemmatized_tweets\n",
        "\n",
        "def stem_tweets(tweets):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tweets = []\n",
        "    for tweet in tweets:\n",
        "        words = word_tokenize(tweet)\n",
        "        stemmed_tweet = ' '.join([stemmer.stem(word) for word in words])\n",
        "        stemmed_tweets.append(stemmed_tweet)\n",
        "    return stemmed_tweets\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nlOsrAzUJeOA"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage with your tweets data\n",
        "lemmatized_tweets = lemmatize_tweets(tweets_without_stopwords)\n",
        "stemmed_tweets = stem_tweets(tweets_without_stopwords)\n",
        "\n",
        "with open('lemmatized_tweets.txt', 'w') as file:\n",
        "    for text in lemmatized_tweets:\n",
        "        file.write(f\"{text}\\n\")\n",
        "\n",
        "with open('stemmed_tweets.txt', 'w') as file:\n",
        "    for text in stemmed_tweets:\n",
        "        file.write(f\"{text}\\n\")"
      ],
      "metadata": {
        "id": "1W8wJ4oEJnQL"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.7 Maintain a corpus of all the words that appeared in the given files."
      ],
      "metadata": {
        "id": "DUqa_-AWKXJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_corpus(tweets):\n",
        "    corpus = []\n",
        "    for tweet in tweets:\n",
        "        words = tweet.split()  # Split each tweet into words\n",
        "        corpus.extend(words)  # Add the words to the corpus\n",
        "    return corpus\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Da09Dy-iKzoz"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "# Assuming 'tweets' is a list of tweet texts\n",
        "corpus = create_corpus(tweets_without_stopwords)\n",
        "# Now 'corpus' contains all the words from the tweets\n",
        "\n",
        "with open('corpus.txt', 'w') as file:\n",
        "    for text in corpus:\n",
        "        file.write(f\"{text}\\n\")"
      ],
      "metadata": {
        "id": "HmnfRBBBK2UQ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3.1\n",
        "\n",
        "\n",
        "Implement a method for \"Edit Distance Algorithm\" , min_edit_dist(target, source, ins_cost, del_cost, sub_cost),\n",
        "to compute the minimum edit distance between a target word and source word. The\n",
        "variables ins_cost, del_cost, sub_cost are insertion, deletion, and substitution costs\n",
        "respectively."
      ],
      "metadata": {
        "id": "4a4WcvQa-YiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_edit_distance(target,source,dist):\n",
        "    for row in dist:\n",
        "        print(\"  \".join(map(str, row)))\n",
        "\n",
        "def min_edit_dist(target, source, ins_cost=1, del_cost=1, sub_cost=2):\n",
        "\n",
        "    n = len(target)\n",
        "    m = len(source)\n",
        "\n",
        "    # Initialize distance matrix\n",
        "    dist = [[0 for _ in range(m + 1)] for _ in range(n + 1)]\n",
        "\n",
        "    # Source prefixes can be transformed into an empty string by deleting all characters\n",
        "    for i in range(1, n + 1):\n",
        "        dist[i][0] = i * del_cost\n",
        "\n",
        "    # Target prefixes can be reached from an empty source prefix by inserting every character\n",
        "    for j in range(1, m + 1):\n",
        "        dist[0][j] = j * ins_cost\n",
        "\n",
        "    for j in range(1, m + 1):\n",
        "        for i in range(1, n + 1):\n",
        "            # Check if substitution is required\n",
        "            if target[i - 1] != source[j - 1]:\n",
        "                sub = sub_cost\n",
        "            else:\n",
        "                sub = 0\n",
        "\n",
        "            dist[i][j] = min(dist[i - 1][j] + del_cost,       # Deletion\n",
        "                             dist[i][j - 1] + ins_cost,       # Insertion\n",
        "                             dist[i - 1][j - 1] + sub)        # Substitution\n",
        "\n",
        "    print_edit_distance(target,source,dist)\n",
        "    return dist[n][m]\n",
        "\n",
        "\n",
        "min_distance = min_edit_dist(\"purpose\", \"possible\")\n",
        "print(\"cost = \"+str(min_distance))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "D2uqLsmX-cKk",
        "outputId": "8b9ba3af-f945-47a3-a649-4db5b89b41ad"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0  1  2  3  4  5  6  7  8\n",
            "1  0  1  2  3  4  5  6  7\n",
            "2  1  2  3  4  5  6  7  8\n",
            "3  2  3  4  5  6  7  8  9\n",
            "4  3  4  5  6  7  8  9  10\n",
            "5  4  3  4  5  6  7  8  9\n",
            "6  5  4  3  4  5  6  7  8\n",
            "7  6  5  4  5  6  7  8  7\n",
            "cost = 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3.2\n",
        "Implement a method, spell_checker(target), which will access the corpora generated\n",
        "in Q1.7 and calculate the minimum edit distance of the target word with all the words in\n",
        "the corpora. Assume insertion and deletion cost as 1 and substitution cost as 0. Return\n",
        "the top five words from the above result.**bold"
      ],
      "metadata": {
        "id": "Nw0NU8Y6CiZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def min_edit_dist(target, source, ins_cost=1, del_cost=1, sub_cost=0):\n",
        "    n = len(target)\n",
        "    m = len(source)\n",
        "    dist = [[0 for _ in range(m + 1)] for _ in range(n + 1)]\n",
        "\n",
        "    for i in range(1, n + 1):\n",
        "        dist[i][0] = i * del_cost\n",
        "\n",
        "    for j in range(1, m + 1):\n",
        "        dist[0][j] = j * ins_cost\n",
        "\n",
        "    for j in range(1, m + 1):\n",
        "        for i in range(1, n + 1):\n",
        "            if target[i - 1] != source[j - 1]:\n",
        "                sub = sub_cost\n",
        "            else:\n",
        "                sub = 0\n",
        "            dist[i][j] = min(dist[i - 1][j] + del_cost,\n",
        "                             dist[i][j - 1] + ins_cost,\n",
        "                             dist[i - 1][j - 1] + sub)\n",
        "    return dist[n][m]\n",
        "\n",
        "def spell_checker(target, corpora):\n",
        "\n",
        "    # if ins=1, del=1, sub=0\n",
        "    distances = [(word, min_edit_dist(target, word)) for word in corpora]\n",
        "\n",
        "\n",
        "    distances.sort(key=lambda x: x[1])\n",
        "    #distances.sort(key=lambda x: x[1], reverse=True)\n",
        "    return distances[:5]\n",
        "\n",
        "def spell_checker_close(target, corpora):\n",
        "\n",
        "    distances = [(word, min_edit_dist(target, word,1,1,2)) for word in corpora]\n",
        "    distances.sort(key=lambda x: x[1])\n",
        "    #distances.sort(key=lambda x: x[1], reverse=True)\n",
        "    return distances[:5]\n",
        "\n"
      ],
      "metadata": {
        "id": "x47YUxzzCkr5"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "will load the corpura from \"corpus.txt\"\n",
        "then will call the spell_checker method.\n",
        "when I test, if the value according to the question : ins=1, del=1, sub=0  then the result will be completely far from the word to search"
      ],
      "metadata": {
        "id": "MdquzYBBOdS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your corpora from a text file\n",
        "corpora = []\n",
        "with open('corpus.txt', 'r') as file:\n",
        "    corpora = file.read().splitlines()\n",
        "\n",
        "# Call the spell_checker function according to the question\n",
        "# ins=1, del=1, sub=0\n",
        "closest_words = spell_checker(\"example\", corpora)\n",
        "\n",
        "\n",
        "# Print the closest words\n",
        "print(closest_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "B0rKACaeMek2",
        "outputId": "6ad73fc6-3aa8-4bc9-f8a1-f69fc9065dc4"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('coaches', 0), ('dietary', 0), ('changes', 0), ('midlife', 0), ('support', 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "but when set the values : ins=1, del=1, sub=2 then it will show closely words"
      ],
      "metadata": {
        "id": "N-Mkj4u9PCUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the spell_checker_close function according to the value:\n",
        "# ins=1, del=1, sub=2\n",
        "closest_words = spell_checker_close(\"example\", corpora)\n",
        "\n",
        "\n",
        "# Print the closest words\n",
        "print(closest_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qU-an3pGOMpR",
        "outputId": "46775e43-a67c-49f8-f9bd-99e2bd3e6261"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('exam', 3), ('exam', 3), ('exam', 3), ('sample', 3), ('apple', 4)]\n"
          ]
        }
      ]
    }
  ]
}